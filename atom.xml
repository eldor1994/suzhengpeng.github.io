<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Suzhengpeng'S Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-09-25T00:03:10+08:00</updated>
 <author>
   <name>Su Zhengpeng</name>
   <uri></uri>
   <email>suzhengpeng@hotmail.com</email>
 </author>

 

 <entry>
   <title>目标跟踪（2）-- 常用的算法测试数据集</title>
   <link href="http://localhost:4000/visual_tracking_02"/>
   <id>http://localhost:4000/visual_tracking_02</id>
   <updated>2018-09-24T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;otb50-和-otb100&quot;&gt;OTB50 和 OTB100&lt;/h3&gt; &lt;p&gt;下载地址：http://cvlab.hanyang.ac.kr/tracker_benchmark/index.html&lt;/p&gt; &lt;p&gt;OTB50拥有50个测试序列，由韩国汉阳大学计算机视觉实验室于2013年发布，因此也被称为OTB2013，而OTB100于2015年发布，亦被称为OTB2015，是对OTB50的扩充，在OTB50的基础上补充了50个新的测试序列。&lt;/p&gt; &lt;h3 id=&quot;vot-数据集&quot;&gt;VOT 数据集&lt;/h3&gt; &lt;p&gt;下载地址：http://www.votchallenge.net/index.html&lt;/p&gt; &lt;p&gt;VOT测试集由VOT目标跟踪挑战赛主办方发布，用以测试相关算法的性能，且每年都会进行更新。&lt;/p&gt; &lt;h3 id=&quot;otb和vot的区别&quot;&gt;OTB和VOT的区别&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因；两个库的评价指标不一样，具体请参考论文；VOT库的序列分辨率普遍较高，这一点后面分析会提到。对于一个tracker，如果论文在两个库(最好是OTB100和VOT2016)上都结果上佳，那肯定是非常优秀的(两个库调参你能调好，我服，认了~~)，如果只跑了一个，个人更偏向于VOT2016，因为序列都是精细标注，且评价指标更好(人家毕竟是竞赛，评价指标发过TPAMI的)，差别最大的地方，OTB有随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后重新初始化，VOT以short-term为主，且认为跟踪检测应该在一起不分离，detecter会多次初始化tracker。&lt;/p&gt; &lt;p&gt;OTB在2013年公开了，对于2013以后的算法是透明的，论文都会去调参，尤其是那些只跑OTB的论文，如果关键参数直接给出还精确到小数点后两位，建议您先实测(人心不古啊~被坑的多了)。VOT竞赛的数据库是每年更新，还动不动就重新标注，动不动就改变评价指标，对当年算法是难度比较大，所以结果相对更可靠。（相信很多人和我一样，看每篇论文都会觉得这个工作太好太重要了，如果没有这篇论文，必定地球爆炸，宇宙重启~~所以就像大家都通过历年ILSVRC竞赛结果为主线了解深度学习的发展一样，第三方的结果更具说服力，所以我也以竞赛排名+是否公开源码+实测性能为标准，优选几个算法分析）&lt;/p&gt; &lt;p&gt;引用自：https://www.leiphone.com/news/201711/d5dMai7835B1uAnR.html&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&quot;数据集分类&quot;&gt;数据集分类&lt;/h3&gt; &lt;p&gt;在OTB数据集中，其以手工标定的方式对测试序列进行了分类。&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;分类&lt;/th&gt; &lt;th&gt;说明&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IV&lt;/td&gt; &lt;td&gt;Illumination Variation - 光线变化 目标所在区域的光线发生显著变化&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SV&lt;/td&gt; &lt;td&gt;Scale Variation - 尺度变化 第一帧和当前帧的边界框的比率超出范围ts，ts&amp;gt; 1（ts = 2）&lt;/td&gt; &lt;/tr&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（1）-- 基于深度学习的目标跟踪</title>
   <link href="http://localhost:4000/visual_tracking_01"/>
   <id>http://localhost:4000/visual_tracking_01</id>
   <updated>2018-09-23T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;使用神经网络进行特征提取&quot;&gt;使用神经网络进行特征提取&lt;/h3&gt; &lt;p&gt;卷积神经网络（CNN） 具有极高的目标特征提取与表达能力，因此将CNN应用于目标跟踪的特征提取阶段，对提高目标跟踪的精度和鲁棒性具有重要的意义。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/01.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt; &lt;center&gt;卷积神经网络的基本结构图&lt;/center&gt; &lt;p&gt;卷积层作特征提取层对目标的特征在不同层次具有不同的描述能力, 卷积层越高, 图像特征分辨率越低，获得的特征也就越抽象, 相反语义信息越丰富, 利用不同卷积层目标特征的不同表达, 针对目标状态有机地结合不同卷积层信息, 对不同卷积层进行区别权重处理, 利用不同目标的描述能力, 对目标跟踪的鲁棒性与精确性有很大的提升。&lt;/p&gt; &lt;p&gt;池化层是特征映射层, 通过对每个特征映射图的局部区域进行加权求和, 增加偏置后通过一个非线性函数在池化层得到新的特征图。池化的作用是: (1) 对特征图进行降维, 避免过拟合; (2) 可以一定程度上缓解目标的形变所引起的问题。&lt;/p&gt; &lt;p&gt;全连接层用于连接所有的特征, 将输出值送给分类器 (如Softmax分类器) , 起到一个分类的作用。&lt;/p&gt; &lt;h3 id=&quot;使用神经网络模拟整个相关滤波过程&quot;&gt;使用神经网络模拟整个相关滤波过程&lt;/h3&gt; &lt;p&gt;相关滤波算法的核心思想是将目标模板与搜索区域内滑动窗口取得的图像块进行相关性匹配, 响应最大位置处对应的图像块为目标图像块。相比传统特征, 深度神经网络提取的图像卷积特征具有良好的抗干扰能力, 在大规模图像分类比赛中取得巨大的成功。&lt;/p&gt; &lt;p&gt;使用神经网络模拟相关滤波的整个过程.在相关滤波中, 需要保存模板信息并提取搜索区域特征, 因此基于相关滤波思想的网络一般都采用孪生网络 (Siamese Network) 结构, 其中一条支路保存目标模板信息, 另一条支路用于搜索区域特征提取, 最后将两部分特征进行相关操作, 得到响应图像 (Response...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（0）-- 视觉目标跟踪技术难点</title>
   <link href="http://localhost:4000/visual_tracking_00"/>
   <id>http://localhost:4000/visual_tracking_00</id>
   <updated>2018-09-21T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;由于目标跟踪过程中目标与环境信息的变化导致目标特征的不断变化, 以及目标跟踪对跟踪速度与精度的要求, 导致目标跟踪存在如下几个主要难点:&lt;/p&gt;

&lt;p&gt;1) 目标外观变化。由于物体活动、非刚体形变 (如人跳跃、行走等) 导致的目标外形发生变化, 或拍摄角度变化导致的目标外观变化等。&lt;/p&gt;

&lt;p&gt;2) 尺度变化。由于拍摄距离等因素导致目标在影像中所占区域大小发生变化。&lt;/p&gt;

&lt;p&gt;3) 环境变化。由于拍摄环境 (如光照、天气等) 变化导致的目标影像成像特点等的变化。&lt;/p&gt;

&lt;p&gt;4) 目标快速运动。由于目标的快速移动导致在影像中的坐标位置发生突变, 影响目标搜索的速度和精度。&lt;/p&gt;

&lt;p&gt;5) 目标遮挡、出视野。由于拍摄中目标被其他物体遮挡导致的特征部分或全部损失以及由于拍摄时目标跳出视野重新跟踪导致的跟踪失败问题。&lt;/p&gt;

&lt;p&gt;6) 成像影响。由于红外摄像仪分辨率较低, 目标分辨率低, 边缘与目标特征不明显, 有时目标与环境差异较小以及任务设备等对焦问题等都会导致目标跟踪时特征的提取困难。&lt;/p&gt;

&lt;p&gt;上述因素对目标跟踪中的目标特征提取以及目标搜索策略具有重大影响, 在实际跟踪过程中, 较为准确地及时处理这些因素所造成的影响才能保证目标跟踪的精确性与鲁棒性。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;[1]葛宝义,左宪章,胡永江.视觉目标跟踪方法研究综述[J].中国图象图形学报,2018,23(08):1091-1107.&lt;/p&gt;
</content>
 </entry>

 

 <entry>
   <title>C6678 PCIE（1）-- 配置地址转换</title>
   <link href="http://localhost:4000/C6678-pcie-01"/>
   <id>http://localhost:4000/C6678-pcie-01</id>
   <updated>2018-09-21T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/img/20180919/2.jpg&quot; width=&quot;700&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1、&lt;strong&gt;Outbound Address Translation&lt;/strong&gt;（OAT）&lt;/p&gt;

&lt;p&gt;存储器域访问PCI域，把设备内部地址映射到PCIE总线上。&lt;/p&gt;

&lt;p&gt;2、&lt;strong&gt;Inbound  Address Translation&lt;/strong&gt;（IAT）&lt;/p&gt;

&lt;p&gt;PCI域访问存储器域、把PCIE总线地址映射到设备内部上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RC访问EP&lt;/strong&gt;: RC存储器域-&amp;gt;outbound-&amp;gt;RC PCI域-&amp;gt;EP PCI域-&amp;gt;inbound-&amp;gt;EP存储器域&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EP访问RC&lt;/strong&gt;：EP存储器域-&amp;gt;outbound-&amp;gt;EP PCI域-&amp;gt;RC PCI域-&amp;gt;inbound-&amp;gt;RC存储器域&lt;/p&gt;

&lt;p&gt;Out即出去，发起访问的一侧，须要进行outbound，去访问对端&lt;/p&gt;

&lt;p&gt;In即进来，被访问的一侧，须要进行inbound，使得对端能够访问&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EP访问RC演示样例（蓝色箭头）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（1）首先，EP须要配置outbound，RC须要inbound(一般RC端不用配)，这样就建立了EP端0x20000000到RC端0x50000000的映射&lt;/p&gt;

&lt;p&gt;（2）在RC端改动0x50000000的内容，EP端能够看到对应的变化。从EP端读/写0x20000000和从RC端读/写0x50000000，结果是一样的&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RC访问EP演示样例（黑色箭头）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（1）首先，RC端须要配置outbound(一般内核中配好)，EP端须要inbound(0x5b000000 inbound到BAR2)，这样就建立了RC端0x20100000（BAR2）到EP端0x5b000000的映射。&lt;/p&gt;

&lt;p&gt;（2）在EP端改动0x5b000000内存的内容，在RC端0x20100000能够看到对应的变化，从RC端读/写0x20100000和从EP端读/写0x5b000000，结果是一样的。&lt;/p&gt;

</content>
 </entry>

 

 <entry>
   <title>C6678 PCIE（0）-- PDK_c667x_2_0_9 测试例程</title>
   <link href="http://localhost:4000/C6678-pcie-00"/>
   <id>http://localhost:4000/C6678-pcie-00</id>
   <updated>2018-09-20T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;在DSP和FPGA的PCIe通信中，DSP常被做End Point 端使用。&lt;/p&gt; &lt;h3 id=&quot;使用命令行导入例程&quot;&gt;使用命令行导入例程&lt;/h3&gt; &lt;p&gt;C6678的Processor SDK，其中提供了相关例程的源代码、RTSC配置文件(.cfg)和一个CCS工程的创建脚本，但没有直接提供CCS工程。需要使用CCS命令行创建。&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;eclipsec -noSplash -data “F:/workspace_v7” -application com.ti.ccstudio.apps.projectCreate -ccs.name pcie_demo -ccs.outputType executable -ccs.device TMS320C66XX.TMS320C6678 -ccs.definePathVariable PDK_INSTALL_PATH C:/ti/pdk_c667x_2_0_9/packages @scope project -rtsc.target ti.targets.elf.C66 -rtsc.platform ti.platforms.evm6678 -rtsc.buildProfile release -ccs.args C:/ti/pdk_c667x_2_0_9/packages/ti/drv/pcie/example/sample/c6678/c66/bios/PCIE_evmc6678_wSoCLib_C66BiosExampleProject.txt&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;参考使用：https://e2echina.ti.com/question_answer/dsp_arm/c6000_multicore/f/53/t/10178&lt;/p&gt; &lt;h3 id=&quot;代码说明&quot;&gt;代码说明&lt;/h3&gt; &lt;h4 id=&quot;任务函数初始化&quot;&gt;任务函数初始化&lt;/h4&gt; &lt;p&gt;在main函数中定义任务函数，并启动BIOS。&lt;/p&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; Task_Params params; Task_Params_init (&amp;amp;params); params.stackSize = 36864;...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（1）-- 现代GPU的体系结构</title>
   <link href="http://localhost:4000/cuda_learning_01"/>
   <id>http://localhost:4000/cuda_learning_01</id>
   <updated>2018-09-19T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/img/20180919/1.jpg&quot; alt=&quot;avatar&quot; /&gt;&lt;/p&gt;

&lt;center&gt;基于CUDA技术的GPU的体系结构&lt;/center&gt;

&lt;p&gt;上图是一个基于CUDA技术的典型GPU体系结构。这种体系结构由一个高度线程化的多核流处理器(Streaming Multiprocessor，SM)阵列组成。在上图中，两个多流处理器（Streaming Multiprocessor，SM）形成一个构建块，然而，在基于CUDA技术的GPU的每一代之间，每个构建块中SM的数量可能不同。此外，图中的每个SM又包含多个流处理器(Streaming Processor，SP)，它们之间共享控制逻辑和指令缓存。每个GPU都带有若干千兆字节(GB)的 &lt;strong&gt;图形双数据速率&lt;/strong&gt; (Graphics Double Data Rate，GDDR) &lt;strong&gt;DRAM&lt;/strong&gt;，在上图中称为 &lt;strong&gt;全局存储器&lt;/strong&gt;(global memory)。GPU中的这些GDDR DRAM完全不同于CPU体系中安装在主板上的系统DRAM，它们主要是用于图形处理的帧缓冲区存储器。在图形应用程序中，它们用来保存视频图像和用于3D渲染的纹理信息；而对于计算，它们可以作为带宽芯片外存储器。尽管比典型系统存储器的延迟要长，大规模并行应用程序通常通过高带宽来弥补时延。&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（0）-- CUDA和GPU</title>
   <link href="http://localhost:4000/cuda_learnning_00"/>
   <id>http://localhost:4000/cuda_learnning_00</id>
   <updated>2018-09-16T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;GPU能够进行并行计算的原因是因为其内部具有成百上千个计算单元，如果我们能够对程序进行拆分，将整个计算过程拆分成大量的独立子任务，这些大量的计算单元就为并行执行这些任务提供了可能性。&lt;/p&gt;

&lt;p&gt;CUDA使用了单指令多线程（Single Instruction Multiple Threads、SIMT）的并行模式。CUDA GPU包含了大量的基础计算单元，这些单元被称为核，每个核都包含了一个逻辑计算单元（ALU）和一个浮点计算单元（FLU）。多个核集成在一起被称为 &lt;strong&gt;多流处理器&lt;/strong&gt;（Stream Multiprocessor、SM）。&lt;/p&gt;

&lt;p&gt;我们将一个计算任务分解为多个子任务，每个子任务被称为线程，多个线程被组织为线程块。线程块被分解为大小与一个SM中核数量相同的 &lt;strong&gt;线程束&lt;/strong&gt;（warp）。每个线程束由一个特定的SM处理器执行。SM处理器的控制单元指挥其所有核同时在一个线程束的每个线程中执行同一个指令,这称为SIMT。&lt;/p&gt;

&lt;p&gt;在GPU上，芯片的大多数空间被分配给了大量组织成SM处理器的运算大院和共享的控制单元。当一个线程束所需的数据不可获得时，SM处理器会转向执行另一个可获得数据的线程束。GPU关注的是整体的运算吞吐量而不是单个核心的执行速度。&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>

 

 <entry>
   <title>初识 Batch Normalization 算法</title>
   <link href="http://localhost:4000/batch-normalization"/>
   <id>http://localhost:4000/batch-normalization</id>
   <updated>2018-09-11T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;p&gt;Batch Normalization 算法于2015年在论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》中发表。主要的作用就是加速网络模型的训练速度。&lt;/p&gt; &lt;h3 id=&quot;bn算法batch-normalization其强大之处如下&quot;&gt;BN算法（Batch Normalization）其强大之处如下&lt;/h3&gt; &lt;p&gt;(1) &lt;strong&gt;你可以选择比较大的初始学习率，让你的训练速度飙涨。&lt;/strong&gt; 以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；&lt;/p&gt; &lt;p&gt;(2) &lt;strong&gt;你再也不用去理会过拟合中drop out、L2正则项参数的选择问题。&lt;/strong&gt; 采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；&lt;/p&gt; &lt;p&gt;(3) &lt;strong&gt;再也不需要使用使用局部响应归一化层了&lt;/strong&gt;（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；&lt;/p&gt; &lt;p&gt;(4)&lt;strong&gt;可以把训练数据彻底打乱&lt;/strong&gt;（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。&lt;/p&gt; &lt;h3 id=&quot;什么是internal-covariate-shift-&quot;&gt;什么是Internal Covariate Shift ？&lt;/h3&gt; &lt;p&gt;Covariate Shift 指的是训练集的数据分布和预测集的数据分布不一致，这样的情况下如果我们在训练集上训练出一个分类器，肯定在预测集上不会取得比较好的效果。这种训练集和预测集样本分布不一致的问题就叫做“covariate shift”现象。&lt;/p&gt; &lt;p&gt;假设x是属于特征空间的某一样本点，y是标签。假设q1(x)是测试集中一个样本点的概率密度，q0(x)是训练集中一个样本点的概率密度。最终我们估计一个条件概率密度$p(y|x，θ)$，它由x和一组参数θ=｛θ1，θ2……θm｝所决定。对于一组参数来说，对应loss(θ)函数评估性能的好坏&lt;/p&gt; &lt;p&gt;综上，当我们找出在q0(x)分布上最优的一组θ’时，能否保证q1(x)上测试时也最好呢？&lt;/p&gt; &lt;p&gt;传统机器学习假设训练集和测试集是独立同分布的，即q0(x)=q1(x)，所以可以推出最优θ’依然可以保证q1(x)最优。但现实当中这个假设往往不成立，伴随新数据产生，老数据会过时，当q0(x)不再等于q1(x)时，就被称作covariate shift。&lt;/p&gt; &lt;p&gt;而Internal Covariate Shit 是在网络层级间发生的Convariate Shift。&lt;/p&gt; &lt;h3 id=&quot;输入的归一化处理&quot;&gt;输入的归一化处理&lt;/h3&gt; &lt;p&gt;简单来说BN算法就是在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。&lt;/p&gt; &lt;hr...</content>
 </entry>

 

 <entry>
   <title>Leetcode 804. Unique Morse Code Words</title>
   <link href="http://localhost:4000/leetcode_804_unique_morse_code_words"/>
   <id>http://localhost:4000/leetcode_804_unique_morse_code_words</id>
   <updated>2018-09-08T00:00:00+08:00</updated>
   <content type="html">&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt; &lt;p&gt;International Morse Code defines a standard encoding where each letter is mapped to a series of dots and dashes, as follows: “a” maps to “.-“, “b” maps to “-…”, “c” maps to “-.-.”, and so on.&lt;/p&gt; &lt;p&gt;For convenience, the full table for the 26 letters of the English...</content>
 </entry>

 

 <entry>
   <title>孪生网络与目标跟踪（2）-- Siamese-FC算法概述</title>
   <link href="http://localhost:4000/siames-network-02"/>
   <id>http://localhost:4000/siames-network-02</id>
   <updated>2018-09-06T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;p&gt;在《Fully-convolutional siamese networks for object tracking》这篇论文中，作者将孪生网络引入到目标跟踪领域 ，即：提出了Siamese-FC算法。在Siamese-FC出现之前，浅层方法（如：KCF 等）的缺点是不能够充分利用端到端学习（End-to-end Learning）的优点，而使用其他网络（非孪生网络）的缺点则是不能实现跟踪对实时性的要求。&lt;/p&gt; &lt;p&gt;Siamese-FC要求首先离线训练一个深度卷积网络，然后以训练好的深度卷积网络作为函数，在跟踪时实时的计算出跟踪的结果。&lt;/p&gt; &lt;p&gt;算法主页：&lt;a href=&quot;http://www.robots.ox.ac.uk/~luca/siamese-fc.html&quot;&gt;http://www.robots.ox.ac.uk/~luca/siamese-fc.html&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;深度相似性学习&quot;&gt;深度相似性学习&lt;/h3&gt; &lt;p&gt;相似性学习是解决跟踪问题的有效方法之一。Siamese-FC算法中提出，通过深度学习得到一个函数$f(z,x)$，将样本图像z和候选图像x在同样尺寸下进行比较，最终得到相似性评分。x与z相似度越高则评分越高，反之越低。为了在候选图像中寻找目标的位置，Siamese-FC要穷举所有可能的位置，并计算相似性评分。$f(z,x)$则通过带有位置标签的视频数据集训练得到。&lt;/p&gt; &lt;h3 id=&quot;完全卷积孪生框架&quot;&gt;完全卷积孪生框架&lt;/h3&gt; &lt;p&gt;基于深度卷积网络进行相似性学习采用的主要方法是孪生框架。在孪生网络中首先对输入x和z分别使用各自独立的变换ϕ，然后对变换结果使用函数$g$，即整个函数f可以表示为：$f(z,x)=g(ϕ(z), ϕ(x))$ 。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180908/01.png&quot; width=&quot;450&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt; &lt;center&gt;完全卷积孪生框架&lt;/center&gt; &lt;p&gt;在Siamese-FC算法中提出的卷积框架，对每一个候选图像x进行完全卷积。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;完全卷积的定义：&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;如果一个函数commutes with translation，则称该函数为为完全卷积。&lt;br /&gt; 这里引入$L_τ$标记为变换算子：$(L_τ x)[u] = x[u-τ]$，函数h如果：$h(L_kτ x) = L_τ h(x)$，则称函数h的映射步长为k的完全卷积。&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;完全卷积的优点：&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;使用完全卷积的优点在于，不需要候选图像和样本图像保持相同的大小，这样就可以将一副足够大的图像作为输入送入网络，以此可以在候选图像中划分子窗，并将子窗做变换，将变换结果一次性完成相似度的计算。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;完全卷积的实现：&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;设ϕ为一个卷积嵌入函数（convolutional embedding function），然后将计算得到的特征映射使用一个互相关层联系到一起。即：&lt;/p&gt;...</content>
 </entry>

 

</feed>