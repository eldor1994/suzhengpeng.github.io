<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Suzhengpeng'S Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-11-02T13:14:04+08:00</updated>
 <author>
   <name>Su Zhengpeng</name>
   <uri></uri>
   <email>suzhengpeng@hotmail.com</email>
 </author>

 

 <entry>
   <title>Hello Pytorch 肆 -- 激活函数</title>
   <link href="http://localhost:4000/hello-pytorch-04"/>
   <id>http://localhost:4000/hello-pytorch-04</id>
   <updated>2018-11-02T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#sigmoid函数&quot; id=&quot;markdown-toc-sigmoid函数&quot;&gt;Sigmoid函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#softmax函数&quot; id=&quot;markdown-toc-softmax函数&quot;&gt;Softmax函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#tanh函数&quot; id=&quot;markdown-toc-tanh函数&quot;&gt;Tanh函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#relu函数&quot; id=&quot;markdown-toc-relu函数&quot;&gt;ReLU函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#leakyrelu&quot; id=&quot;markdown-toc-leakyrelu&quot;&gt;LeakyReLU&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。激活函数的使用给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。&lt;/p&gt; &lt;h3 id=&quot;sigmoid函数&quot;&gt;Sigmoid函数&lt;/h3&gt; &lt;p&gt;Sigmoid函数又叫Logistic函数，是皮埃尔·弗朗索瓦·韦吕勒在1844年在研究它与人口增长的关系时命名的。广义Logistic曲线可以模仿一些情况人口增长（P）的 S 形曲线。起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢；最后，达到成熟时增加停止。&lt;/p&gt; &lt;p&gt;之所以叫Sigmoid，是因为函数的图像很想一个字母S。从图像上我们可以观察到一些直观的特性：函数的取值在0-1之间，且在0.5处为中心对称，并且越靠近x=0的取值斜率越大。&lt;/p&gt; &lt;p&gt;Sigmoid函数在物理意义上最为接近生物神经元。(0, 1) 的输出还可以被表示作概率，或用于输入的归一化，代表性的如Sigmoid交叉熵损失函数。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;计算公式：&lt;/strong&gt;&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}&lt;/script&gt; &lt;p&gt;&lt;strong&gt;函数图像：&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;img width=&quot;400px&quot; src=&quot;/img/201811/Sigmoid.png&quot; /&gt;&lt;/p&gt; &lt;h3 id=&quot;softmax函数&quot;&gt;Softmax函数&lt;/h3&gt; &lt;p&gt;Softmax函数，或称归一化指数函数。用于多分类过程中将多个神经元的输出，映射到（0,1）区间内。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;计算公式：&lt;/strong&gt;&lt;/p&gt; &lt;script type=&quot;math/tex;...</content>
 </entry>

 

 <entry>
   <title>Hello Pytorch 叁 -- 简单理解生成对抗网络（GAN）</title>
   <link href="http://localhost:4000/hello-pytorch-03"/>
   <id>http://localhost:4000/hello-pytorch-03</id>
   <updated>2018-10-29T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#零和博弈思想&quot; id=&quot;markdown-toc-零和博弈思想&quot;&gt;零和博弈思想&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#优化目标函数&quot; id=&quot;markdown-toc-优化目标函数&quot;&gt;优化目标函数&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#优化方法&quot; id=&quot;markdown-toc-优化方法&quot;&gt;优化方法&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#优化过程&quot; id=&quot;markdown-toc-优化过程&quot;&gt;优化过程&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;零和博弈思想&quot;&gt;零和博弈思想&lt;/h3&gt; &lt;p&gt;GAN（生产对抗网络，Generative Adversarial Nets）的思想是是一种二人零和博弈思想（two-player game），指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。&lt;/p&gt; &lt;p&gt;引申到GAN里面就是可以看成，GAN中有两个这样的博弈者，一个是生成模型（Generator）用来生成一张真实的图片，另一个是判别模型（Discriminator）判别一张图片是生成出来的还是真实存在的。整个博弈的过程，就变成了如下模式：生成模型生成一些图片-&amp;gt;判别模型学习区分生成的图片和真实图片-&amp;gt;生成模型根据判别模型改进自己，生成新的图片-&amp;gt;···· 这个博弈的过程直至生成模型与判别模型无法提高自己——即判别模型无法判断一张图片是生成出来的还是真实的而结束，此时生成模型就会成为一个完美的模型。这种博弈式的训练过程，如果采用神经网络作为模型类型，则被称为生成对抗网络（GAN）&lt;/p&gt; &lt;h3 id=&quot;优化目标函数&quot;&gt;优化目标函数&lt;/h3&gt; &lt;p&gt;这里设x为训练样本，z为随机噪声。训练得到的生成模型为G，判别模型为D。G(z)将这个随机噪声转化为与x具有相同数据结构的图像，D(x)为判别结果是0-1范围内的一个实数。&lt;/p&gt; &lt;p&gt;生成模型的目标是让判别模型无法区分真实图片与生成图片，那么整个的优化目标函数如下：&lt;/p&gt; &lt;p&gt;&lt;img width=&quot;500px&quot; src=&quot;/img/201810/04.jpg&quot; /&gt;&lt;/p&gt; &lt;h4 id=&quot;优化方法&quot;&gt;优化方法&lt;/h4&gt; &lt;p&gt;对这个最大最小化目标函数，最直观的处理办法就是分别对D和G进行交互迭代，固定G，优化D，一段时间后，固定D再优化G，直到过程收敛。&lt;/p&gt; &lt;p&gt;首先，固定G，优化D：&lt;/p&gt; &lt;p&gt;&lt;img width=&quot;500px&quot; src=&quot;/img/201810/05.jpg&quot; /&gt;&lt;/p&gt; &lt;p&gt;优化判别模型D的时候，与生成模型G无关，G(z)相当于已经得到的生成样本。当真实样本x（真实样本标签为1）输入的时候，希望得到的判别结果D(x)越接近于1越好，同时目标函数越大越好。对于生成样本G(z)（其标签为0），希望得到的判别结果D(G(z))越接近于0越好，也就是使1-D(G(z))越接近于1越好，而目标函数越大越好。因此将两者同时输入，希望得到的目标函数的和越大越好。&lt;/p&gt; &lt;p&gt;然后，固定D，优化G：&lt;/p&gt; &lt;p&gt;&lt;img width=&quot;400px&quot; src=&quot;/img/201810/06.jpg&quot; /&gt;&lt;/p&gt;...</content>
 </entry>

 

 <entry>
   <title>Hello Pytorch 贰 -- 常用损失函数</title>
   <link href="http://localhost:4000/hello-pytorch-02"/>
   <id>http://localhost:4000/hello-pytorch-02</id>
   <updated>2018-10-20T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#损失函数&quot; id=&quot;markdown-toc-损失函数&quot;&gt;损失函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#l1loss-l1-范数损失函数&quot; id=&quot;markdown-toc-l1loss-l1-范数损失函数&quot;&gt;L1Loss L1-范数损失函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#mseloss-均方差损失函数&quot; id=&quot;markdown-toc-mseloss-均方差损失函数&quot;&gt;MSELoss 均方差损失函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#crossentropyloss-交叉熵损失函数&quot; id=&quot;markdown-toc-crossentropyloss-交叉熵损失函数&quot;&gt;CrossEntropyLoss 交叉熵损失函数&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;官方文档：https://pytorch.org/docs/stable/nn.html#loss-functions&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&quot;损失函数&quot;&gt;损失函数&lt;/h3&gt; &lt;p&gt;损失函数（loss function）是一个非负实值函数，用来计算模型的预测值f(x)与真实值Y的差异程度。&lt;/p&gt; &lt;h3 id=&quot;l1loss-l1-范数损失函数&quot;&gt;L1Loss L1-范数损失函数&lt;/h3&gt; &lt;p&gt;计算公式：&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,&lt;/script&gt; &lt;p&gt;其中 N 为batch的大小。如果...</content>
 </entry>

 

 <entry>
   <title>Hello Pytorch 壹 -- 卷积层原理及实现</title>
   <link href="http://localhost:4000/hello-pytorch-01"/>
   <id>http://localhost:4000/hello-pytorch-01</id>
   <updated>2018-10-20T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#卷积与互相关计算&quot; id=&quot;markdown-toc-卷积与互相关计算&quot;&gt;卷积与互相关计算&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#卷积&quot; id=&quot;markdown-toc-卷积&quot;&gt;卷积&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#动画演示&quot; id=&quot;markdown-toc-动画演示&quot;&gt;动画演示&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#二维卷积-torchnnconv2d&quot; id=&quot;markdown-toc-二维卷积-torchnnconv2d&quot;&gt;二维卷积 torch.nn.Conv2d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#一维卷积-torchnnconv1d&quot; id=&quot;markdown-toc-一维卷积-torchnnconv1d&quot;&gt;一维卷积 torch.nn.Conv1d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#三维卷积-torchnnconv3d&quot; id=&quot;markdown-toc-三维卷积-torchnnconv3d&quot;&gt;三维卷积 torch.nn.Conv3d&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#反卷积转置卷积&quot; id=&quot;markdown-toc-反卷积转置卷积&quot;&gt;反卷积（转置卷积）&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#动画演示-1&quot; id=&quot;markdown-toc-动画演示-1&quot;&gt;动画演示&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#二维反卷积-torchnnconvtranspose2d&quot; id=&quot;markdown-toc-二维反卷积-torchnnconvtranspose2d&quot;&gt;二维反卷积 torch.nn.ConvTranspose2d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#一维反卷积-torchnnconvtranspose1d&quot; id=&quot;markdown-toc-一维反卷积-torchnnconvtranspose1d&quot;&gt;一维反卷积 torch.nn.ConvTranspose1d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#三维反卷积-torchnnconvtranspose3d&quot; id=&quot;markdown-toc-三维反卷积-torchnnconvtranspose3d&quot;&gt;三维反卷积 torch.nn.ConvTranspose3d&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt;...</content>
 </entry>

 

 <entry>
   <title>Hello Pytorch 零 -- 搭建年轻人的第一个神经网络：LeNet</title>
   <link href="http://localhost:4000/hello-pytorch-00"/>
   <id>http://localhost:4000/hello-pytorch-00</id>
   <updated>2018-10-19T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#经典网络模型lenet&quot; id=&quot;markdown-toc-经典网络模型lenet&quot;&gt;经典网络模型：LeNet&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#网络结构&quot; id=&quot;markdown-toc-网络结构&quot;&gt;网络结构&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#理解网络&quot; id=&quot;markdown-toc-理解网络&quot;&gt;理解网络&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#cifar-10训练集&quot; id=&quot;markdown-toc-cifar-10训练集&quot;&gt;CIFAR-10训练集&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#搭建网络&quot; id=&quot;markdown-toc-搭建网络&quot;&gt;搭建网络&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#代码示例&quot; id=&quot;markdown-toc-代码示例&quot;&gt;代码示例&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#导入所需的包&quot; id=&quot;markdown-toc-导入所需的包&quot;&gt;导入所需的包&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#定义对数据的预处理&quot; id=&quot;markdown-toc-定义对数据的预处理&quot;&gt;定义对数据的预处理&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#定义训练集&quot; id=&quot;markdown-toc-定义训练集&quot;&gt;定义训练集&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#定义测试集&quot; id=&quot;markdown-toc-定义测试集&quot;&gt;定义测试集&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#定义网络&quot; id=&quot;markdown-toc-定义网络&quot;&gt;定义网络&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#定义损失函数和优化器&quot; id=&quot;markdown-toc-定义损失函数和优化器&quot;&gt;定义损失函数和优化器&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#训练网络&quot; id=&quot;markdown-toc-训练网络&quot;&gt;训练网络&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#测试效果&quot; id=&quot;markdown-toc-测试效果&quot;&gt;测试效果&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（6）-- 流</title>
   <link href="http://localhost:4000/cuda_learning_06"/>
   <id>http://localhost:4000/cuda_learning_06</id>
   <updated>2018-10-16T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#关于流&quot; id=&quot;markdown-toc-关于流&quot;&gt;关于流&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#流并行&quot; id=&quot;markdown-toc-流并行&quot;&gt;流并行&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#页锁内存的概念&quot; id=&quot;markdown-toc-页锁内存的概念&quot;&gt;页锁内存的概念&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#流的运行机制&quot; id=&quot;markdown-toc-流的运行机制&quot;&gt;流的运行机制&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#多个流的调度&quot; id=&quot;markdown-toc-多个流的调度&quot;&gt;多个流的调度&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;关于流&quot;&gt;关于流&lt;/h3&gt; &lt;p&gt;在GPU上，流是执行异步并行的主要载体。在GPU上，每个流都可以看作是一个独立的任务，每个流中的代码操作顺序执行。&lt;/p&gt; &lt;p&gt;关于CPU和GPU的异步并行详见：&lt;a href=&quot;/cuda_learning_05&quot; target=&quot;_blank&quot;&gt;《CUDA并行编程学习（5）– 异步并行》&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;流并行&quot;&gt;流并行&lt;/h3&gt; &lt;p&gt;流并行是指我们可以创建多个流来执行多个任务， 但每个流都是一个需要按照顺序执行的操作队列。&lt;/p&gt; &lt;h4 id=&quot;页锁内存的概念&quot;&gt;页锁内存的概念&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;使用页锁内存的意义&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;想要实现流并行，需要主机使用一块固定的内存，一般称之为页锁内存（Page Locked Memory），而一般情况下经过分配得到的内存为可分页内存（Pagable Memory）。而可分页内存面临着重定位的问题，因此使用可分页内存进行复制时，复制可能执行两次操作：从可分页内存复制到一块“临时”页锁定内存，然后从页锁定内存复制到GPU。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;页锁内存的性质&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;页锁内存 （Page Locked Memory）又称为固定内存（Pinned Memory）或者不可分页内存。 操作系统将不会对这块内存分页并交换到磁盘上，从而确保了该内存始终驻留在物理内存中，因为这块内存将不会被破坏或者重新定位。 由于gpu知道内存的物理地址，因此可以通过“直接内存访问（Direct Memory...</content>
 </entry>

 

 <entry>
   <title>OpenCV GPU 模块学习 （2）GPU的流操作</title>
   <link href="http://localhost:4000/opencv-gpu-module-2"/>
   <id>http://localhost:4000/opencv-gpu-module-2</id>
   <updated>2018-10-14T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;软件信息&lt;/strong&gt;：&lt;br /&gt; OpenCV Version : 2.4.13.6&lt;br /&gt; CUDA Version : 8.0&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;OpenCV gpu::Stream 手册&lt;/strong&gt;&lt;br /&gt; https://docs.opencv.org/2.4.13.6/modules/gpu/doc/data_structures.html#gpu::Stream&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;OpenCV 通过 gpu::Stream 类封装了一个异步调用。&lt;/p&gt; &lt;p&gt;在OpenCV的gpu模块中提供的一些函数具有附加 gpu::Stream 参数的重载，这些函数可以进行初始化工作，启动GPU核函数，并且可以在结果计算完成之前返回。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;编程思想：&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;新建一个Stream队列，使用 gpu::Strean::enqueueUpload() 将需要操作的数据放入队列中，然后在队列中进行所需要的操作，可以通过gpu::Stream::queryIfComplete() 和 gpu::Stream::waitForCompletion() 判断和检测操作是否完成。等待队列中的操作完成，使用 gpu::strean::enqueueDownload() 将数据返回。相较于默认的数据传输方法，使用Stream队列实现了数据的异步调用，不需要等待数据全部上传完成后再对数据进行操作，只需要在确定所需的数据，即可在上传数据的同时对数据进行操作，达到GPU计算的同时进行GPU与CPU之间的数据交换，从而达到提升效率的目。&lt;/p&gt; &lt;p&gt;关于CPU和GPU异步并行的概念和意义详见：&lt;a href=&quot;/cuda_learning_05&quot; target=&quot;_blank&quot;&gt;《CUDA并行编程学习（5）– 异步并行》&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;类的内容：&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（5）-- 异步并行</title>
   <link href="http://localhost:4000/cuda_learning_05"/>
   <id>http://localhost:4000/cuda_learning_05</id>
   <updated>2018-10-13T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;h3 id=&quot;什么是异步并行&quot;&gt;什么是异步并行&lt;/h3&gt; &lt;p&gt;异步并行，即：控制GPU在没有完成任务请求之前返回CPU。&lt;/p&gt; &lt;h3 id=&quot;异步并行的意义&quot;&gt;异步并行的意义&lt;/h3&gt; &lt;p&gt;默认情况下，CPU在调用GPU函数时，主机将等待调用完成并返回结果，这就意味着在GPU计算的时候CPU并不会执行任何任务。&lt;/p&gt; &lt;p&gt;而在执行异步并行时，处于同一数据流内的计算与数据传输是依次进行的，但一个流内的计算可以和另一个流的数据传输可以同时进行。&lt;/p&gt; &lt;p&gt;1、通过异步执行就能够使GPU中的执行单元与存储器单元同时工作，更好地压榨GPU的性能。&lt;/p&gt; &lt;p&gt;2、当GPU在进行计算或者数据传输时就返回给主机线程，主机不必等待GPU运行完毕就可以进行进行一些计算，更好地压榨了CPU的性能。&lt;/p&gt; &lt;h3 id=&quot;例程&quot;&gt;例程&lt;/h3&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; #include &quot;cuda_runtime.h&quot; #include &quot;device_launch_parameters.h&quot; #include &quot;stdio.h&quot; #include &quot;memory.h&quot; #include &quot;time.h&quot; #include &quot;helper_timer.h&quot; // 定义核函数 __global__ void increment_kernel(int *g_data, int inc_value) { int idx = blockIdx.x*blockDim.x + threadIdx.x; g_data[idx] =...</content>
 </entry>

 

 <entry>
   <title>OpenCV GPU 模块学习 （1）读取GPU设备信息</title>
   <link href="http://localhost:4000/opencv-gpu-module-1"/>
   <id>http://localhost:4000/opencv-gpu-module-1</id>
   <updated>2018-10-10T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;软件信息&lt;/strong&gt;：&lt;br /&gt; OpenCV Version : 2.4.13.6&lt;br /&gt; CUDA Version : 8.0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;在OpenCV GPU 模块中，提供了一些列函数，来读取GPU信息。函数列表如下所列：&lt;/p&gt; &lt;p&gt;https://docs.opencv.org/2.4.13.6/modules/gpu/doc/initalization_and_information.html&lt;/p&gt; &lt;p&gt;通过一个例程来测试这些函数的使用方法。&lt;/p&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; #include &quot;opencv2/opencv.hpp&quot; #include &quot;opencv2/gpu/gpu.hpp&quot; int main() { // 获取设备的数量 int num_devices = cv::gpu::getCudaEnabledDeviceCount(); // 检测设备的数量，保证设备数量不小于0 if (num_devices &amp;lt;= 0) { std::cerr &amp;lt;&amp;lt;...</content>
 </entry>

 

 <entry>
   <title>编译GPU版本的OpenCV</title>
   <link href="http://localhost:4000/compile-opencv-with-cuda-on-ubuntu"/>
   <id>http://localhost:4000/compile-opencv-with-cuda-on-ubuntu</id>
   <updated>2018-10-09T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;p&gt;编译GPU版本的OpenCV与CPU版本的OpenCV并没有太大的区别，只有几个需要在细节上注意的地方。除此之外，可以参考： &lt;a href=&quot;./compile-opencv-on-ubuntu&quot; target=&quot;_blank&quot;&gt;《从源码开始，在Ubuntu上安装OpenCV》&lt;/a&gt; 这篇文章讲述的方法进行。&lt;/p&gt; &lt;p&gt;在安装之前，要确认已经在电脑中正确安装了 CUDA 开发包和 NVCC 编译器。&lt;/p&gt; &lt;h3 id=&quot;配置cmake&quot;&gt;配置CMAKE&lt;/h3&gt; &lt;p&gt;WITH_CUDA ON&lt;/p&gt; &lt;p&gt;WITH_CUBLAS ON&lt;/p&gt; &lt;p&gt;WITH_CUFFT ON&lt;/p&gt; &lt;p&gt;WITH_NVCUVIDs ON&lt;/p&gt; &lt;p&gt;CUDA_FAST_MATH ON&lt;/p&gt; &lt;p&gt;CUDA_GENERATION Auto&lt;/p&gt; &lt;p&gt;其他的选项默认即可。&lt;/p&gt; &lt;h3 id=&quot;可能会出现的bug&quot;&gt;可能会出现的BUG&lt;/h3&gt; &lt;p&gt;在使用CUDA 9.0的时候，编译OpenCV可能会如下的错误，而且无论是 OpenCV 2.x 还是 OpenCV 3.x 都有出现的可能，但是解决方法略有差别。&lt;/p&gt; &lt;p&gt;所以，推荐使用 CUDA 8.0 配置&lt;/p&gt; &lt;p&gt;&lt;strong&gt;错误信息&lt;/strong&gt; ：&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;CMake...</content>
 </entry>

 

</feed>