<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Suzhengpeng'S Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-09-30T15:51:16+08:00</updated>
 <author>
   <name>Su Zhengpeng</name>
   <uri></uri>
   <email>suzhengpeng@hotmail.com</email>
 </author>

 

 <entry>
   <title>OpenCV 4.0 Alpha 带来的新变化</title>
   <link href="http://localhost:4000/hello-opencv-4"/>
   <id>http://localhost:4000/hello-opencv-4</id>
   <updated>2018-09-30T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;h3 id=&quot;新特性&quot;&gt;新特性&lt;/h3&gt; &lt;p&gt;最近OpenCV发布了全新的OpenCV4.0 alpha版 （下载地址 ~ https://github.com/opencv/opencv/archive/4.0.0-alpha.zip ）&lt;/p&gt; &lt;p&gt;根据其在官网上的说明，4.0是在3.4之上发布的全新版本，对3.4进行了优化，并修复了其中存在的若干BUG。&lt;/p&gt; &lt;blockquote&gt; &lt;ol&gt; &lt;li&gt;ONNX parser has been added to OpenCV DNN module. It supports various classification networks, such as AlexNet, Inception v2, Resnet, VGG etc. The tiny YOLO v2 object detection network is also partially...</content>
 </entry>

 

 <entry>
   <title>从源码开始，在Ubuntu上安装OpenCV</title>
   <link href="http://localhost:4000/compile-opencv-on-ubuntu"/>
   <id>http://localhost:4000/compile-opencv-on-ubuntu</id>
   <updated>2018-09-29T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#所需工具&quot; id=&quot;markdown-toc-所需工具&quot;&gt;所需工具&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#从源码开始安装&quot; id=&quot;markdown-toc-从源码开始安装&quot;&gt;从源码开始安装&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#使用cmake-生成-makefile&quot; id=&quot;markdown-toc-使用cmake-生成-makefile&quot;&gt;使用CMake 生成 makefile&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用make-编译并安装&quot; id=&quot;markdown-toc-使用make-编译并安装&quot;&gt;使用Make 编译并安装&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#编写demo测试安装结果&quot; id=&quot;markdown-toc-编写demo测试安装结果&quot;&gt;编写Demo测试安装结果&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#代码清单&quot; id=&quot;markdown-toc-代码清单&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#编写cmakeliststxt&quot; id=&quot;markdown-toc-编写cmakeliststxt&quot;&gt;编写CMakeLists.txt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用cmakemake完成编译&quot; id=&quot;markdown-toc-使用cmakemake完成编译&quot;&gt;使用CMake+Make完成编译&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;所需工具&quot;&gt;所需工具&lt;/h3&gt; &lt;p&gt;Ubuntu 系统：系统版本 Ubuntu 16.04.4 LTS&lt;/p&gt; &lt;p&gt;Cmake： cmake version 3.5.1&lt;/p&gt; &lt;p&gt;Make：...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（4）-- 内存共享 Share Memory</title>
   <link href="http://localhost:4000/cuda_learning_04"/>
   <id>http://localhost:4000/cuda_learning_04</id>
   <updated>2018-09-27T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#共享内存的优点&quot; id=&quot;markdown-toc-共享内存的优点&quot;&gt;共享内存的优点&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#创建共享内存&quot; id=&quot;markdown-toc-创建共享内存&quot;&gt;创建共享内存&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#例程matrix-multiplication&quot; id=&quot;markdown-toc-例程matrix-multiplication&quot;&gt;例程：Matrix Multiplication&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#不使用共享内存&quot; id=&quot;markdown-toc-不使用共享内存&quot;&gt;不使用共享内存&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#算法说明&quot; id=&quot;markdown-toc-算法说明&quot;&gt;算法说明&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码清单&quot; id=&quot;markdown-toc-代码清单&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用共享内存&quot; id=&quot;markdown-toc-使用共享内存&quot;&gt;使用共享内存&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#算法说明-1&quot; id=&quot;markdown-toc-算法说明-1&quot;&gt;算法说明&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码清单-1&quot; id=&quot;markdown-toc-代码清单-1&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码下载&quot; id=&quot;markdown-toc-代码下载&quot;&gt;代码下载&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h4 id=&quot;共享内存的优点&quot;&gt;共享内存的优点&lt;/h4&gt; &lt;p&gt;使用共享内存的优点在于，一般情况下，共享内存比全局内存的访问速度更快。任何可以使用共享内存的地方，都应该将全局内存替换为共享内存。&lt;/p&gt; &lt;h4 id=&quot;创建共享内存&quot;&gt;创建共享内存&lt;/h4&gt;...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（3）-- 将CUDA并行模型扩展到二维空间</title>
   <link href="http://localhost:4000/cuda_learning_03"/>
   <id>http://localhost:4000/cuda_learning_03</id>
   <updated>2018-09-26T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#二维内核的启动方法&quot; id=&quot;markdown-toc-二维内核的启动方法&quot;&gt;二维内核的启动方法&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#指定二维线程块的大小&quot; id=&quot;markdown-toc-指定二维线程块的大小&quot;&gt;指定二维线程块的大小&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#计算xy方向上的线程块数&quot; id=&quot;markdown-toc-计算xy方向上的线程块数&quot;&gt;计算x，y方向上的线程块数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#启动内核&quot; id=&quot;markdown-toc-启动内核&quot;&gt;启动内核&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#二维内核函数&quot; id=&quot;markdown-toc-二维内核函数&quot;&gt;二维内核函数&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#典型的二维内核函数&quot; id=&quot;markdown-toc-典型的二维内核函数&quot;&gt;典型的二维内核函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#计算图像数据的二维内核函数&quot; id=&quot;markdown-toc-计算图像数据的二维内核函数&quot;&gt;计算图像数据的二维内核函数&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;二维内核的启动方法&quot;&gt;二维内核的启动方法&lt;/h3&gt; &lt;p&gt;设一幅图像有w列，h行。在x方向使用TX个线程，在Y方向使用TY个线程&lt;/p&gt; &lt;h4 id=&quot;指定二维线程块的大小&quot;&gt;指定二维线程块的大小&lt;/h4&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; dim3 blockSize(TX , TY); &lt;/code&gt; &lt;/pre&gt; &lt;h4 id=&quot;计算xy方向上的线程块数&quot;&gt;计算x，y方向上的线程块数&lt;/h4&gt; &lt;pre&gt; &lt;code...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（2）-- 使用CUDA计算一个数组的距离值</title>
   <link href="http://localhost:4000/cuda_learning_02"/>
   <id>http://localhost:4000/cuda_learning_02</id>
   <updated>2018-09-26T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;代码下载：https://github.com/myurtoglu/cudaforengineers/tree/master/dist_v1_cuda&lt;/strong&gt;&lt;br /&gt; 基于《CUDA高性能并行计算》中的例程&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; #include &amp;lt;stdio.h&amp;gt; #define N 64 #define TPB 32 &lt;/code&gt; &lt;/pre&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; int main() { const float ref = 0.5f; float *d_out = 0; cudaMalloc(&amp;amp;d_out, N*sizeof(float)); distanceKernel&amp;lt;&amp;lt;&amp;lt;N/TPB, TPB&amp;gt;&amp;gt;&amp;gt;(d_out, ref, N); cudaFree(d_out); // Free the memory return 0; } &lt;/code&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（3）-- Mean-shift 与目标跟踪[1]</title>
   <link href="http://localhost:4000/visual_tracking_03"/>
   <id>http://localhost:4000/visual_tracking_03</id>
   <updated>2018-09-24T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;mean-shift-算法理解&quot;&gt;Mean Shift 算法理解&lt;/h3&gt; &lt;p&gt;基本概念：沿着密度上升方向寻找聚簇点&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/02.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt; &lt;p&gt;设在d维空间$R_d$中，划定一个有N个样本点、半径为k的高维球区域$S_k$，初始随机确定一个中心点center，N个样本点记为集合M，认为这些点属于聚簇C。计算以中心点为起点落在球内的样本点（$x_i$）为终点的向量 。计算整个圆形空间内所有向量的和，得到一个偏移向量。将中心点center沿着shift的方向移动到偏移向量的终点，移动距离即偏移向量的模。重复移动，又得到一个新的偏移向量。如此重复下去，直到偏移向量的大小满足设定的阈值要求，以此保证收敛到概率密度最大得地方。也就是最稠密的地方。 &lt;img src=&quot;/img/20180923/03.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;计算偏移向量&lt;/center&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/04.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;移动中心点&lt;/center&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/05.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;得到最终结果：收敛到概率密度最大得地方&lt;/center&gt; &lt;!-- ### 经典Mean-shift框架 ### ASMS目标跟踪算法 Github 主页：https://github.com/vojirt/asms ASMS是VOT2015官方推荐的实时算法，平均帧率125FPS。在经典mean-shift框架下加入了尺度估计，经典颜色直方图特征，加入了两个先验(尺度不剧变+可能偏最大)作为正则项，和反向尺度一致性检查。 #### 经典MS算法存在的问题 1.在跟踪过程中位置错误时可以在MS迭代中进行自我纠正，而尺度错误则不会。 &lt;img src=&quot;/img/20180923/03.PNG&quot; width=&quot;400&quot; /&gt; &lt;center&gt;Illustration of the scale ambiguity problem 尺度歧义问题说明&lt;/center&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（2）-- 常用的算法测试数据集</title>
   <link href="http://localhost:4000/visual_tracking_02"/>
   <id>http://localhost:4000/visual_tracking_02</id>
   <updated>2018-09-24T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;otb50-和-otb100&quot;&gt;OTB50 和 OTB100&lt;/h3&gt; &lt;p&gt;下载地址：http://cvlab.hanyang.ac.kr/tracker_benchmark/index.html&lt;/p&gt; &lt;p&gt;OTB50拥有50个测试序列，由韩国汉阳大学计算机视觉实验室于2013年发布，因此也被称为OTB2013，而OTB100于2015年发布，亦被称为OTB2015，是对OTB50的扩充，在OTB50的基础上补充了50个新的测试序列。&lt;/p&gt; &lt;h3 id=&quot;vot-数据集&quot;&gt;VOT 数据集&lt;/h3&gt; &lt;p&gt;下载地址：http://www.votchallenge.net/index.html&lt;/p&gt; &lt;p&gt;VOT测试集由VOT目标跟踪挑战赛主办方发布，用以测试相关算法的性能，且每年都会进行更新。&lt;/p&gt; &lt;h3 id=&quot;otb和vot的区别&quot;&gt;OTB和VOT的区别&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因；两个库的评价指标不一样，具体请参考论文；VOT库的序列分辨率普遍较高，这一点后面分析会提到。对于一个tracker，如果论文在两个库(最好是OTB100和VOT2016)上都结果上佳，那肯定是非常优秀的(两个库调参你能调好，我服，认了~~)，如果只跑了一个，个人更偏向于VOT2016，因为序列都是精细标注，且评价指标更好(人家毕竟是竞赛，评价指标发过TPAMI的)，差别最大的地方，OTB有随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后重新初始化，VOT以short-term为主，且认为跟踪检测应该在一起不分离，detecter会多次初始化tracker。&lt;/p&gt; &lt;p&gt;OTB在2013年公开了，对于2013以后的算法是透明的，论文都会去调参，尤其是那些只跑OTB的论文，如果关键参数直接给出还精确到小数点后两位，建议您先实测(人心不古啊~被坑的多了)。VOT竞赛的数据库是每年更新，还动不动就重新标注，动不动就改变评价指标，对当年算法是难度比较大，所以结果相对更可靠。（相信很多人和我一样，看每篇论文都会觉得这个工作太好太重要了，如果没有这篇论文，必定地球爆炸，宇宙重启~~所以就像大家都通过历年ILSVRC竞赛结果为主线了解深度学习的发展一样，第三方的结果更具说服力，所以我也以竞赛排名+是否公开源码+实测性能为标准，优选几个算法分析）&lt;/p&gt; &lt;p&gt;引用自：https://www.zhihu.com/question/26493945/answer/156025576&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&quot;数据集分类&quot;&gt;数据集分类&lt;/h3&gt; &lt;p&gt;在OTB数据集中，其以手工标定的方式对测试序列进行了分类。&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;分类&lt;/th&gt; &lt;th&gt;说明&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IV&lt;/td&gt; &lt;td&gt;Illumination Variation - 光线变化 目标所在区域的光线发生显著变化&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SV&lt;/td&gt; &lt;td&gt;Scale Variation - 尺度变化 第一帧和当前帧的边界框的比率超出范围ts，ts&amp;gt; 1（ts = 2）&lt;/td&gt; &lt;/tr&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（1）-- 基于深度学习的目标跟踪</title>
   <link href="http://localhost:4000/visual_tracking_01"/>
   <id>http://localhost:4000/visual_tracking_01</id>
   <updated>2018-09-23T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;使用神经网络进行特征提取&quot;&gt;使用神经网络进行特征提取&lt;/h3&gt; &lt;p&gt;卷积神经网络（CNN） 具有极高的目标特征提取与表达能力，因此将CNN应用于目标跟踪的特征提取阶段，对提高目标跟踪的精度和鲁棒性具有重要的意义。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/01.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt; &lt;center&gt;卷积神经网络的基本结构图&lt;/center&gt; &lt;p&gt;卷积层作特征提取层对目标的特征在不同层次具有不同的描述能力, 卷积层越高, 图像特征分辨率越低，获得的特征也就越抽象, 相反语义信息越丰富, 利用不同卷积层目标特征的不同表达, 针对目标状态有机地结合不同卷积层信息, 对不同卷积层进行区别权重处理, 利用不同目标的描述能力, 对目标跟踪的鲁棒性与精确性有很大的提升。&lt;/p&gt; &lt;p&gt;池化层是特征映射层, 通过对每个特征映射图的局部区域进行加权求和, 增加偏置后通过一个非线性函数在池化层得到新的特征图。池化的作用是: (1) 对特征图进行降维, 避免过拟合; (2) 可以一定程度上缓解目标的形变所引起的问题。&lt;/p&gt; &lt;p&gt;全连接层用于连接所有的特征, 将输出值送给分类器 (如Softmax分类器) , 起到一个分类的作用。&lt;/p&gt; &lt;h3 id=&quot;使用神经网络模拟整个相关滤波过程&quot;&gt;使用神经网络模拟整个相关滤波过程&lt;/h3&gt; &lt;p&gt;相关滤波算法的核心思想是将目标模板与搜索区域内滑动窗口取得的图像块进行相关性匹配, 响应最大位置处对应的图像块为目标图像块。相比传统特征, 深度神经网络提取的图像卷积特征具有良好的抗干扰能力, 在大规模图像分类比赛中取得巨大的成功。&lt;/p&gt; &lt;p&gt;使用神经网络模拟相关滤波的整个过程.在相关滤波中, 需要保存模板信息并提取搜索区域特征, 因此基于相关滤波思想的网络一般都采用孪生网络 (Siamese Network) 结构, 其中一条支路保存目标模板信息, 另一条支路用于搜索区域特征提取, 最后将两部分特征进行相关操作, 得到响应图像 (Response...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（0）-- 视觉目标跟踪技术难点</title>
   <link href="http://localhost:4000/visual_tracking_00"/>
   <id>http://localhost:4000/visual_tracking_00</id>
   <updated>2018-09-21T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;由于目标跟踪过程中目标与环境信息的变化导致目标特征的不断变化, 以及目标跟踪对跟踪速度与精度的要求, 导致目标跟踪存在如下几个主要难点:&lt;/p&gt;

&lt;p&gt;1) 目标外观变化。由于物体活动、非刚体形变 (如人跳跃、行走等) 导致的目标外形发生变化, 或拍摄角度变化导致的目标外观变化等。&lt;/p&gt;

&lt;p&gt;2) 尺度变化。由于拍摄距离等因素导致目标在影像中所占区域大小发生变化。&lt;/p&gt;

&lt;p&gt;3) 环境变化。由于拍摄环境 (如光照、天气等) 变化导致的目标影像成像特点等的变化。&lt;/p&gt;

&lt;p&gt;4) 目标快速运动。由于目标的快速移动导致在影像中的坐标位置发生突变, 影响目标搜索的速度和精度。&lt;/p&gt;

&lt;p&gt;5) 目标遮挡、出视野。由于拍摄中目标被其他物体遮挡导致的特征部分或全部损失以及由于拍摄时目标跳出视野重新跟踪导致的跟踪失败问题。&lt;/p&gt;

&lt;p&gt;6) 成像影响。由于红外摄像仪分辨率较低, 目标分辨率低, 边缘与目标特征不明显, 有时目标与环境差异较小以及任务设备等对焦问题等都会导致目标跟踪时特征的提取困难。&lt;/p&gt;

&lt;p&gt;上述因素对目标跟踪中的目标特征提取以及目标搜索策略具有重大影响, 在实际跟踪过程中, 较为准确地及时处理这些因素所造成的影响才能保证目标跟踪的精确性与鲁棒性。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;[1]葛宝义,左宪章,胡永江.视觉目标跟踪方法研究综述[J].中国图象图形学报,2018,23(08):1091-1107.&lt;/p&gt;
</content>
 </entry>

 

 <entry>
   <title>C6678 PCIE（1）-- 配置地址转换</title>
   <link href="http://localhost:4000/C6678-pcie-01"/>
   <id>http://localhost:4000/C6678-pcie-01</id>
   <updated>2018-09-21T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/img/20180919/2.jpg&quot; width=&quot;700&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1、&lt;strong&gt;Outbound Address Translation&lt;/strong&gt;（OAT）&lt;/p&gt;

&lt;p&gt;存储器域访问PCI域，把设备内部地址映射到PCIE总线上。&lt;/p&gt;

&lt;p&gt;2、&lt;strong&gt;Inbound  Address Translation&lt;/strong&gt;（IAT）&lt;/p&gt;

&lt;p&gt;PCI域访问存储器域、把PCIE总线地址映射到设备内部上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RC访问EP&lt;/strong&gt;: RC存储器域-&amp;gt;outbound-&amp;gt;RC PCI域-&amp;gt;EP PCI域-&amp;gt;inbound-&amp;gt;EP存储器域&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EP访问RC&lt;/strong&gt;：EP存储器域-&amp;gt;outbound-&amp;gt;EP PCI域-&amp;gt;RC PCI域-&amp;gt;inbound-&amp;gt;RC存储器域&lt;/p&gt;

&lt;p&gt;Out即出去，发起访问的一侧，须要进行outbound，去访问对端&lt;/p&gt;

&lt;p&gt;In即进来，被访问的一侧，须要进行inbound，使得对端能够访问&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EP访问RC演示样例（蓝色箭头）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（1）首先，EP须要配置outbound，RC须要inbound(一般RC端不用配)，这样就建立了EP端0x20000000到RC端0x50000000的映射&lt;/p&gt;

&lt;p&gt;（2）在RC端改动0x50000000的内容，EP端能够看到对应的变化。从EP端读/写0x20000000和从RC端读/写0x50000000，结果是一样的&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RC访问EP演示样例（黑色箭头）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（1）首先，RC端须要配置outbound(一般内核中配好)，EP端须要inbound(0x5b000000 inbound到BAR2)，这样就建立了RC端0x20100000（BAR2）到EP端0x5b000000的映射。&lt;/p&gt;

&lt;p&gt;（2）在EP端改动0x5b000000内存的内容，在RC端0x20100000能够看到对应的变化，从RC端读/写0x20100000和从EP端读/写0x5b000000，结果是一样的。&lt;/p&gt;

</content>
 </entry>

 

</feed>