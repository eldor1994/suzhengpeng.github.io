<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Suzhengpeng'S Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-09-08T21:56:35+08:00</updated>
 <author>
   <name>Su Zhengpeng</name>
   <uri></uri>
   <email>suzhengpeng@hotmail.com</email>
 </author>

 

 <entry>
   <title>Leetcode 804. Unique Morse Code Words</title>
   <link href="http://localhost:4000/leetcode_804_unique_morse_code_words"/>
   <id>http://localhost:4000/leetcode_804_unique_morse_code_words</id>
   <updated>2018-09-08T00:00:00+08:00</updated>
   <content type="html">&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt; &lt;p&gt;International Morse Code defines a standard encoding where each letter is mapped to a series of dots and dashes, as follows: “a” maps to “.-“, “b” maps to “-…”, “c” maps to “-.-.”, and so on.&lt;/p&gt; &lt;p&gt;For convenience, the full table for the 26 letters of the English...</content>
 </entry>

 

 <entry>
   <title>初识全连接层</title>
   <link href="http://localhost:4000/fully-connected-layer"/>
   <id>http://localhost:4000/fully-connected-layer</id>
   <updated>2018-09-06T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt;

&lt;h4 id=&quot;概述&quot;&gt;概述&lt;/h4&gt;

&lt;p&gt;全连接层 Fully Connected Layer 一般位于整个卷积神经网络的最后，负责将卷积输出的二维特征图转化成一维的一个向量，由此实现了端到端的学习过程（即：输入一张图像或一段语音，输出一个向量或信息）。全连接层的每一个结点都与上一层的所有结点相连因而称之为全连接层。由于其全相连的特性，一般全连接层的参数也是最多的。&lt;/p&gt;

&lt;h4 id=&quot;主要作用&quot;&gt;主要作用&lt;/h4&gt;

&lt;p&gt;全连接层的主要作用就是将前层（卷积、池化等层）计算得到的特征空间映射样本标记空间。简单的说就是将特征表示整合成一个值，其优点在于减少特征位置对于分类结果的影响，提高了整个网络的鲁棒性。&lt;/p&gt;

&lt;p&gt;在知乎上有这样一个回答说的很形象。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;假设你是一只小蚂蚁，你的任务是找小面包。你的视野还比较窄，只能看到很小一片区域。当你找到一片小面包之后，你不知道你找到的是不是全部的小面包，所以你们全部的蚂蚁开了个会，把所有的小面包都拿出来分享了。全连接层就是这个蚂蚁大会~如果提前告诉你全世界就只有一块小面包，你找到之后也就掌握了全部的信息，这种情况下也就没必要引入fc层了&lt;br /&gt;
&lt;strong&gt;作者：田star&lt;/strong&gt;  链接：https://www.zhihu.com/question/41037974/answer/150552142&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;实现方式&quot;&gt;实现方式&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/20180906/01.jpg&quot; width=&quot;450&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt;
&lt;center&gt;全连接层的计算方式&lt;/center&gt;

&lt;p&gt;如上图所示，一个网络在全连接层之前，生成了5@3×3的特征映射，我们需要只需要使用五个卷积核去和激活函数的输出进行卷积运算，在将五个输出的值相加即可得到一个全连接层的输出值。如果结果是N维的向量，则需要N×5个3×3的卷积核。再加上求和运算对应的权值，参数的数量是非常可观的，由此一般只在网络的之后使用全连接层。&lt;/p&gt;

&lt;p&gt;参考资料：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;strong&gt;CNN 入门讲解：什么是全连接层（Fully Connected Layer）?&lt;/strong&gt; https://zhuanlan.zhihu.com/p/33841176&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;全连接层的作用是什么？ - 魏秀参 的回答&lt;/strong&gt; https://www.zhihu.com/question/41037974&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;解释一下全连接层&lt;/strong&gt; https://blog.csdn.net/u011021773/article/details/78121359&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
</content>
 </entry>

 

 <entry>
   <title>孪生网络与目标跟踪（1）-- 孪生网络的基本结构</title>
   <link href="http://localhost:4000/siames-network-01"/>
   <id>http://localhost:4000/siames-network-01</id>
   <updated>2018-09-03T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;Yann LeCun 在他2005年的论文Learning a Similarity Metric Discriminatively, with Application to Face Verificatio中提出使用孪生网络做人脸鉴定。&lt;/p&gt; &lt;h3 id=&quot;核心思想&quot;&gt;核心思想&lt;/h3&gt; &lt;p&gt;这种方法的核心思想在于：通过训练一个网络得到一个函数，这个函数可以将输入映射到目标空间，然后在目标空间中计算输入空间的”语义（semantic）”距离，如使用 $L_1$范数。整个网络的学习过程就是将同一个人的人脸图像的输出最小化，不同的人的人脸图像输出最大化。将原始图像映射进入目标空间的工具就是一个对几何畸变（geometric distortions）具有鲁棒性的卷积网络。更确切的说就是有一系列的函数$G_W(X)$使用W作为参数，我们的目标就是寻找合适的参数W，使得相似矩阵$E_W(X_1,X_2) = ||G_W(X_1) - G_W(X_2) ||$ 在$X_1$和$X_2$属于同一分类（即：人脸图像属于同一个人）时计算得到的结果最大，反之则最小。根据孪生网络的特点，对于输入$X_1$和$X_2$其对应的G和W是相同的。&lt;/p&gt; &lt;h3 id=&quot;设计目标&quot;&gt;设计目标&lt;/h3&gt; &lt;p&gt;设计的可训练系统要在训练时能够同时最小化错误接受率（false accepts）和错误拒绝率（false reject）。同时可将原始图片（raw images）映射到低维空间，以此计算两个输入的相似度距离（即：相似度度量 similarity metric）。&lt;/p&gt; &lt;h3 id=&quot;孪生网络结构&quot;&gt;孪生网络结构&lt;/h3&gt; &lt;p&gt;&lt;img src=&quot;/img/20180904/01.png&quot; width=&quot;450&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt; &lt;center&gt;孪生网络结构&lt;/center&gt; &lt;h4 id=&quot;整体框架&quot;&gt;整体框架&lt;/h4&gt; &lt;p&gt;$X_1$和$X_2$分别为两个待学习的样本对，Y是样本对的标签（$X_1$ 与 $X_2$ 属于同一个人时 Y为0，反之为1）。$G_W(X_1)$和$G_W(X_2)$为输入的原始图像映射到的低维度空间。而$E_W(X_1,X_2) = ||G_W(X_1) - G_W(X_2)...</content>
 </entry>

 

 <entry>
   <title>孪生网络与目标跟踪（0）-- 孪生网络的基本概念</title>
   <link href="http://localhost:4000/siames-network-00"/>
   <id>http://localhost:4000/siames-network-00</id>
   <updated>2018-09-02T00:00:00+08:00</updated>
   <content type="html">&lt;h3 id=&quot;孪生网络的概念&quot;&gt;孪生网络的概念&lt;/h3&gt; &lt;p&gt;孪生网络即 Siamese Networks ,Siamese 意为“孪生的、连体的”，孪生网络的“孪生”在于两个各自独立的神经网络，具有共同的权值。这种神经网络由Yann Lecun 大神在1993年提出，其论文发表在1994年的NIPS上。孪生网络是用于度量学习的监督网络模型（Supervised models）。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180902/01.jpg&quot; width=&quot;300&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt; &lt;center&gt;共享权值的孪生神经网络&lt;/center&gt; &lt;h5 id=&quot;共享权值的意义&quot;&gt;共享权值的意义？&lt;/h5&gt; &lt;p&gt;两个神经网络共享权值即意味着两个网络的权值完全相同，这意为这在通过代码实现的时候只需要实现一个即可，而不需要再去实现另一个。&lt;/p&gt; &lt;h5 id=&quot;伪孪生网络&quot;&gt;伪孪生网络&lt;/h5&gt; &lt;p&gt;伪孪生网络即：pseudo-siamese network，存在两个神经独立的神经网络，相比于孪生神经网络他们之间并不共享权值。如下图所示：&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180902/02.jpg&quot; width=&quot;300&quot; alt=&quot;孪生网络&quot; /&gt;&lt;/p&gt; &lt;center&gt;伪孪生网络&lt;/center&gt; &lt;p&gt;两边可以是不同的神经网络（如一个是lstm，一个是cnn），也可以是相同类型的神经网络。&lt;/p&gt; &lt;h3 id=&quot;孪生网络的使用&quot;&gt;孪生网络的使用&lt;/h3&gt; &lt;p&gt;一个孪生网络有两个输入，简单来说，孪生网络的直接用途就是衡量这两个输入的差异程度（或者说相似程度）。将两个输入分别Feed 进入两个神经网络，将两个输入映射到新的空间，将输入在新的空间中表示，通过Loss Function来计算两个输入的差异程度（或相似程度）。&lt;/p&gt; &lt;h5 id=&quot;应用场景&quot;&gt;应用场景&lt;/h5&gt; &lt;p&gt;孪生网络适用的场景主要为判断两个输入的相似程度。如：比较两个句子的语义相似程度比较、手写字体的笔迹鉴定、以及目标跟踪领域中的目标（前景）和背景区分。&lt;/p&gt; &lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt; &lt;p&gt;孪生网络的训练目标就是使相似的输入距离尽可能的小，使不同的输入尽可能的大。&lt;/p&gt; &lt;p&gt;孪生网络的两个网络将输入分别转换成向量值，通过计算向量的距离得到输入的差异程度。&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;参考资料：&lt;/p&gt; &lt;blockquote&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Siamese network...</content>
 </entry>

 

 <entry>
   <title>Leetcode 91. Decode Ways</title>
   <link href="http://localhost:4000/leetcode_91_decode_ways"/>
   <id>http://localhost:4000/leetcode_91_decode_ways</id>
   <updated>2018-09-02T00:00:00+08:00</updated>
   <content type="html">&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt; &lt;p&gt;A message containing letters from A-Z is being encoded to numbers using the following mapping:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‘A’ -&amp;gt; 1&lt;br /&gt; ‘B’ -&amp;gt; 2&lt;br /&gt; …&lt;br /&gt; ‘Z’ -&amp;gt; 26&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Given a non-empty string containing only digits, determine the total number of ways to decode it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt;...</content>
 </entry>

 

 <entry>
   <title>Opencv3 Mat 结构像素值存储细节</title>
   <link href="http://localhost:4000/opencv-mat-pixel-struct"/>
   <id>http://localhost:4000/opencv-mat-pixel-struct</id>
   <updated>2018-03-02T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;在Opencv3 中常使用Mat 结构保存图像或数据。&lt;/p&gt; &lt;h3 id=&quot;深度-depth&quot;&gt;深度 Depth&lt;/h3&gt; &lt;p&gt;Mat 结构的深度类型一般有以下6种&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;类型&lt;/th&gt; &lt;th&gt;解释&lt;/th&gt; &lt;th&gt;变量类型&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CV_8U&lt;/td&gt; &lt;td&gt;单通道8位无符号整数&lt;/td&gt; &lt;td&gt;unsigned char&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CV_8S&lt;/td&gt; &lt;td&gt;单通道8位有符号整数&lt;/td&gt; &lt;td&gt;char&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CV_16S&lt;/td&gt; &lt;td&gt;单通道16位有符号整数&lt;/td&gt; &lt;td&gt;short int&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CV_32S&lt;/td&gt; &lt;td&gt;单通道32位有符号整数&lt;/td&gt; &lt;td&gt;int&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CV_32F&lt;/td&gt; &lt;td&gt;单通道32位单精度浮点数&lt;/td&gt; &lt;td&gt;float&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CV_64F&lt;/td&gt; &lt;td&gt;单通道64位双精度浮点数&lt;/td&gt; &lt;td&gt;double&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;...</content>
 </entry>

 

 <entry>
   <title>Hello World</title>
   <link href="http://localhost:4000/hello-world"/>
   <id>http://localhost:4000/hello-world</id>
   <updated>2017-04-20T00:00:00+08:00</updated>
   <content type="html">&lt;p&gt;&lt;img src=&quot;http://suzhengpeng.com/img/cover/20170422135316.jpg&quot; alt=&quot;avatar&quot; /&gt;
经历了两天的奋战，终于在Github上用jekyll重建了自己的博客。欢迎光临我的博客。&lt;/p&gt;
</content>
 </entry>

 

</feed>