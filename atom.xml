<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Suzhengpeng'S Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-10-10T09:16:31+08:00</updated>
 <author>
   <name>Su Zhengpeng</name>
   <uri></uri>
   <email>suzhengpeng@hotmail.com</email>
 </author>

 

 <entry>
   <title>编译GPU版本的OpenCV</title>
   <link href="http://localhost:4000/compile-opencv-with-cuda-on-ubuntu"/>
   <id>http://localhost:4000/compile-opencv-with-cuda-on-ubuntu</id>
   <updated>2018-10-09T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;p&gt;编译GPU版本的OpenCV与CPU版本的OpenCV并没有太大的区别，只有几个需要在细节上注意的地方。除此之外，可以参考： &lt;a href=&quot;./compile-opencv-on-ubuntu&quot; target=&quot;_blank&quot;&gt;《从源码开始，在Ubuntu上安装OpenCV》&lt;/a&gt; 这篇文章讲述的方法进行。&lt;/p&gt; &lt;p&gt;在安装之前，要确认已经在电脑中正确安装了 CUDA 开发包和 NVCC 编译器。&lt;/p&gt; &lt;h3 id=&quot;配置cmake&quot;&gt;配置CMAKE&lt;/h3&gt; &lt;p&gt;WITH_CUDA ON&lt;/p&gt; &lt;p&gt;WITH_CUBLAS ON&lt;/p&gt; &lt;p&gt;WITH_CUFFT ON&lt;/p&gt; &lt;p&gt;WITH_NVCUVIDs ON&lt;/p&gt; &lt;p&gt;CUDA_FAST_MATH ON&lt;/p&gt; &lt;p&gt;其他的选项默认即可。&lt;/p&gt; &lt;h3 id=&quot;可能会出现的bug&quot;&gt;可能会出现的BUG&lt;/h3&gt; &lt;p&gt;在使用CUDA 9.0的时候，编译OpenCV可能会如下的错误，而且无论是 OpenCV 2.x 还是 OpenCV 3.x 都有出现的可能，但是解决方法略有差别。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;错误信息&lt;/strong&gt; ：&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;CMake Error: The following variables are used...</content>
 </entry>

 

 <entry>
   <title>『穿越河西 慢游敦煌』 第一日 - 兰州</title>
   <link href="http://localhost:4000/2018_1001_1"/>
   <id>http://localhost:4000/2018_1001_1</id>
   <updated>2018-10-08T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引
{:toc}  --&gt;

&lt;!-- ---- --&gt;

&lt;p&gt;&lt;img src=&quot;/img/201810/IMG_5620.JPG&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://weibo.com/ttarticle/p/show?id=2309404293363368122056&quot; target=&quot;_blank&quot;&gt;&lt;center&gt;进入微博头条文章查看&lt;/center&gt;&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>

 

 <entry>
   <title>OpenCV 4.0 Alpha 带来的新变化</title>
   <link href="http://localhost:4000/hello-opencv-4"/>
   <id>http://localhost:4000/hello-opencv-4</id>
   <updated>2018-09-30T00:00:00+08:00</updated>
   <content type="html">&lt;!-- * 索引 {:toc} --&gt; &lt;hr /&gt; &lt;h3 id=&quot;新特性&quot;&gt;新特性&lt;/h3&gt; &lt;p&gt;最近OpenCV发布了全新的OpenCV4.0 alpha版 （下载地址 ~ https://github.com/opencv/opencv/archive/4.0.0-alpha.zip ）&lt;/p&gt; &lt;p&gt;根据其在官网上的说明，4.0是在3.4之上发布的全新版本，对3.4进行了优化，并修复了其中存在的若干BUG。&lt;/p&gt; &lt;blockquote&gt; &lt;ol&gt; &lt;li&gt;ONNX parser has been added to OpenCV DNN module. It supports various classification networks, such as AlexNet, Inception v2, Resnet, VGG etc. The tiny YOLO v2 object detection network is also partially...</content>
 </entry>

 

 <entry>
   <title>从源码开始，在Ubuntu上安装OpenCV</title>
   <link href="http://localhost:4000/compile-opencv-on-ubuntu"/>
   <id>http://localhost:4000/compile-opencv-on-ubuntu</id>
   <updated>2018-09-29T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#所需工具&quot; id=&quot;markdown-toc-所需工具&quot;&gt;所需工具&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#从源码开始安装&quot; id=&quot;markdown-toc-从源码开始安装&quot;&gt;从源码开始安装&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#使用cmake-生成-makefile&quot; id=&quot;markdown-toc-使用cmake-生成-makefile&quot;&gt;使用CMake 生成 makefile&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用make-编译并安装&quot; id=&quot;markdown-toc-使用make-编译并安装&quot;&gt;使用Make 编译并安装&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#编写demo测试安装结果&quot; id=&quot;markdown-toc-编写demo测试安装结果&quot;&gt;编写Demo测试安装结果&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#代码清单&quot; id=&quot;markdown-toc-代码清单&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#编写cmakeliststxt&quot; id=&quot;markdown-toc-编写cmakeliststxt&quot;&gt;编写CMakeLists.txt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用cmakemake完成编译&quot; id=&quot;markdown-toc-使用cmakemake完成编译&quot;&gt;使用CMake+Make完成编译&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;所需工具&quot;&gt;所需工具&lt;/h3&gt; &lt;p&gt;Ubuntu 系统：系统版本 Ubuntu 16.04.4 LTS&lt;/p&gt; &lt;p&gt;Cmake： cmake version 3.5.1&lt;/p&gt; &lt;p&gt;Make：...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（4）-- 内存共享 Share Memory</title>
   <link href="http://localhost:4000/cuda_learning_04"/>
   <id>http://localhost:4000/cuda_learning_04</id>
   <updated>2018-09-27T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#共享内存的优点&quot; id=&quot;markdown-toc-共享内存的优点&quot;&gt;共享内存的优点&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#创建共享内存&quot; id=&quot;markdown-toc-创建共享内存&quot;&gt;创建共享内存&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#例程matrix-multiplication&quot; id=&quot;markdown-toc-例程matrix-multiplication&quot;&gt;例程：Matrix Multiplication&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#不使用共享内存&quot; id=&quot;markdown-toc-不使用共享内存&quot;&gt;不使用共享内存&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#算法说明&quot; id=&quot;markdown-toc-算法说明&quot;&gt;算法说明&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码清单&quot; id=&quot;markdown-toc-代码清单&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#使用共享内存&quot; id=&quot;markdown-toc-使用共享内存&quot;&gt;使用共享内存&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#算法说明-1&quot; id=&quot;markdown-toc-算法说明-1&quot;&gt;算法说明&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码清单-1&quot; id=&quot;markdown-toc-代码清单-1&quot;&gt;代码清单&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#代码下载&quot; id=&quot;markdown-toc-代码下载&quot;&gt;代码下载&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h4 id=&quot;共享内存的优点&quot;&gt;共享内存的优点&lt;/h4&gt; &lt;p&gt;使用共享内存的优点在于，一般情况下，共享内存比全局内存的访问速度更快。任何可以使用共享内存的地方，都应该将全局内存替换为共享内存。&lt;/p&gt; &lt;h4 id=&quot;创建共享内存&quot;&gt;创建共享内存&lt;/h4&gt;...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（3）-- 将CUDA并行模型扩展到二维空间</title>
   <link href="http://localhost:4000/cuda_learning_03"/>
   <id>http://localhost:4000/cuda_learning_03</id>
   <updated>2018-09-26T00:00:00+08:00</updated>
   <content type="html">&lt;ul id=&quot;markdown-toc&quot;&gt; &lt;li&gt;&lt;a href=&quot;#二维内核的启动方法&quot; id=&quot;markdown-toc-二维内核的启动方法&quot;&gt;二维内核的启动方法&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#指定二维线程块的大小&quot; id=&quot;markdown-toc-指定二维线程块的大小&quot;&gt;指定二维线程块的大小&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#计算xy方向上的线程块数&quot; id=&quot;markdown-toc-计算xy方向上的线程块数&quot;&gt;计算x，y方向上的线程块数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#启动内核&quot; id=&quot;markdown-toc-启动内核&quot;&gt;启动内核&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#二维内核函数&quot; id=&quot;markdown-toc-二维内核函数&quot;&gt;二维内核函数&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;#典型的二维内核函数&quot; id=&quot;markdown-toc-典型的二维内核函数&quot;&gt;典型的二维内核函数&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#计算图像数据的二维内核函数&quot; id=&quot;markdown-toc-计算图像数据的二维内核函数&quot;&gt;计算图像数据的二维内核函数&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&quot;二维内核的启动方法&quot;&gt;二维内核的启动方法&lt;/h3&gt; &lt;p&gt;设一幅图像有w列，h行。在x方向使用TX个线程，在Y方向使用TY个线程&lt;/p&gt; &lt;h4 id=&quot;指定二维线程块的大小&quot;&gt;指定二维线程块的大小&lt;/h4&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; dim3 blockSize(TX , TY); &lt;/code&gt; &lt;/pre&gt; &lt;h4 id=&quot;计算xy方向上的线程块数&quot;&gt;计算x，y方向上的线程块数&lt;/h4&gt; &lt;pre&gt; &lt;code...</content>
 </entry>

 

 <entry>
   <title>CUDA并行编程学习（2）-- 使用CUDA计算一个数组的距离值</title>
   <link href="http://localhost:4000/cuda_learning_02"/>
   <id>http://localhost:4000/cuda_learning_02</id>
   <updated>2018-09-26T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;代码下载：https://github.com/myurtoglu/cudaforengineers/tree/master/dist_v1_cuda&lt;/strong&gt;&lt;br /&gt; 基于《CUDA高性能并行计算》中的例程&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; #include &amp;lt;stdio.h&amp;gt; #define N 64 #define TPB 32 &lt;/code&gt; &lt;/pre&gt; &lt;pre&gt; &lt;code class=&quot;cpp&quot;&gt; int main() { const float ref = 0.5f; float *d_out = 0; cudaMalloc(&amp;amp;d_out, N*sizeof(float)); distanceKernel&amp;lt;&amp;lt;&amp;lt;N/TPB, TPB&amp;gt;&amp;gt;&amp;gt;(d_out, ref, N); cudaFree(d_out); // Free the memory return 0; } &lt;/code&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（3）-- Mean-shift 与目标跟踪[1]</title>
   <link href="http://localhost:4000/visual_tracking_03"/>
   <id>http://localhost:4000/visual_tracking_03</id>
   <updated>2018-09-24T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;mean-shift-算法理解&quot;&gt;Mean Shift 算法理解&lt;/h3&gt; &lt;p&gt;基本概念：沿着密度上升方向寻找聚簇点&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/02.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt; &lt;p&gt;设在d维空间$R_d$中，划定一个有N个样本点、半径为k的高维球区域$S_k$，初始随机确定一个中心点center，N个样本点记为集合M，认为这些点属于聚簇C。计算以中心点为起点落在球内的样本点（$x_i$）为终点的向量 。计算整个圆形空间内所有向量的和，得到一个偏移向量。将中心点center沿着shift的方向移动到偏移向量的终点，移动距离即偏移向量的模。重复移动，又得到一个新的偏移向量。如此重复下去，直到偏移向量的大小满足设定的阈值要求，以此保证收敛到概率密度最大得地方。也就是最稠密的地方。 &lt;img src=&quot;/img/20180923/03.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;计算偏移向量&lt;/center&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/04.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;移动中心点&lt;/center&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/05.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt; &lt;center&gt;得到最终结果：收敛到概率密度最大得地方&lt;/center&gt; &lt;!-- ### 经典Mean-shift框架 ### ASMS目标跟踪算法 Github 主页：https://github.com/vojirt/asms ASMS是VOT2015官方推荐的实时算法，平均帧率125FPS。在经典mean-shift框架下加入了尺度估计，经典颜色直方图特征，加入了两个先验(尺度不剧变+可能偏最大)作为正则项，和反向尺度一致性检查。 #### 经典MS算法存在的问题 1.在跟踪过程中位置错误时可以在MS迭代中进行自我纠正，而尺度错误则不会。 &lt;img src=&quot;/img/20180923/03.PNG&quot; width=&quot;400&quot; /&gt; &lt;center&gt;Illustration of the scale ambiguity problem 尺度歧义问题说明&lt;/center&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（2）-- 常用的算法测试数据集</title>
   <link href="http://localhost:4000/visual_tracking_02"/>
   <id>http://localhost:4000/visual_tracking_02</id>
   <updated>2018-09-24T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;otb50-和-otb100&quot;&gt;OTB50 和 OTB100&lt;/h3&gt; &lt;p&gt;下载地址：http://cvlab.hanyang.ac.kr/tracker_benchmark/index.html&lt;/p&gt; &lt;p&gt;OTB50拥有50个测试序列，由韩国汉阳大学计算机视觉实验室于2013年发布，因此也被称为OTB2013，而OTB100于2015年发布，亦被称为OTB2015，是对OTB50的扩充，在OTB50的基础上补充了50个新的测试序列。&lt;/p&gt; &lt;h3 id=&quot;vot-数据集&quot;&gt;VOT 数据集&lt;/h3&gt; &lt;p&gt;下载地址：http://www.votchallenge.net/index.html&lt;/p&gt; &lt;p&gt;VOT测试集由VOT目标跟踪挑战赛主办方发布，用以测试相关算法的性能，且每年都会进行更新。&lt;/p&gt; &lt;h3 id=&quot;otb和vot的区别&quot;&gt;OTB和VOT的区别&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因；两个库的评价指标不一样，具体请参考论文；VOT库的序列分辨率普遍较高，这一点后面分析会提到。对于一个tracker，如果论文在两个库(最好是OTB100和VOT2016)上都结果上佳，那肯定是非常优秀的(两个库调参你能调好，我服，认了~~)，如果只跑了一个，个人更偏向于VOT2016，因为序列都是精细标注，且评价指标更好(人家毕竟是竞赛，评价指标发过TPAMI的)，差别最大的地方，OTB有随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后重新初始化，VOT以short-term为主，且认为跟踪检测应该在一起不分离，detecter会多次初始化tracker。&lt;/p&gt; &lt;p&gt;OTB在2013年公开了，对于2013以后的算法是透明的，论文都会去调参，尤其是那些只跑OTB的论文，如果关键参数直接给出还精确到小数点后两位，建议您先实测(人心不古啊~被坑的多了)。VOT竞赛的数据库是每年更新，还动不动就重新标注，动不动就改变评价指标，对当年算法是难度比较大，所以结果相对更可靠。（相信很多人和我一样，看每篇论文都会觉得这个工作太好太重要了，如果没有这篇论文，必定地球爆炸，宇宙重启~~所以就像大家都通过历年ILSVRC竞赛结果为主线了解深度学习的发展一样，第三方的结果更具说服力，所以我也以竞赛排名+是否公开源码+实测性能为标准，优选几个算法分析）&lt;/p&gt; &lt;p&gt;引用自：https://www.zhihu.com/question/26493945/answer/156025576&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&quot;数据集分类&quot;&gt;数据集分类&lt;/h3&gt; &lt;p&gt;在OTB数据集中，其以手工标定的方式对测试序列进行了分类。&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;分类&lt;/th&gt; &lt;th&gt;说明&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IV&lt;/td&gt; &lt;td&gt;Illumination Variation - 光线变化 目标所在区域的光线发生显著变化&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SV&lt;/td&gt; &lt;td&gt;Scale Variation - 尺度变化 第一帧和当前帧的边界框的比率超出范围ts，ts&amp;gt; 1（ts = 2）&lt;/td&gt; &lt;/tr&gt;...</content>
 </entry>

 

 <entry>
   <title>目标跟踪（1）-- 基于深度学习的目标跟踪</title>
   <link href="http://localhost:4000/visual_tracking_01"/>
   <id>http://localhost:4000/visual_tracking_01</id>
   <updated>2018-09-23T00:00:00+08:00</updated>
   <content type="html">&lt;hr /&gt; &lt;h3 id=&quot;使用神经网络进行特征提取&quot;&gt;使用神经网络进行特征提取&lt;/h3&gt; &lt;p&gt;卷积神经网络（CNN） 具有极高的目标特征提取与表达能力，因此将CNN应用于目标跟踪的特征提取阶段，对提高目标跟踪的精度和鲁棒性具有重要的意义。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/img/20180923/01.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt; &lt;center&gt;卷积神经网络的基本结构图&lt;/center&gt; &lt;p&gt;卷积层作特征提取层对目标的特征在不同层次具有不同的描述能力, 卷积层越高, 图像特征分辨率越低，获得的特征也就越抽象, 相反语义信息越丰富, 利用不同卷积层目标特征的不同表达, 针对目标状态有机地结合不同卷积层信息, 对不同卷积层进行区别权重处理, 利用不同目标的描述能力, 对目标跟踪的鲁棒性与精确性有很大的提升。&lt;/p&gt; &lt;p&gt;池化层是特征映射层, 通过对每个特征映射图的局部区域进行加权求和, 增加偏置后通过一个非线性函数在池化层得到新的特征图。池化的作用是: (1) 对特征图进行降维, 避免过拟合; (2) 可以一定程度上缓解目标的形变所引起的问题。&lt;/p&gt; &lt;p&gt;全连接层用于连接所有的特征, 将输出值送给分类器 (如Softmax分类器) , 起到一个分类的作用。&lt;/p&gt; &lt;h3 id=&quot;使用神经网络模拟整个相关滤波过程&quot;&gt;使用神经网络模拟整个相关滤波过程&lt;/h3&gt; &lt;p&gt;相关滤波算法的核心思想是将目标模板与搜索区域内滑动窗口取得的图像块进行相关性匹配, 响应最大位置处对应的图像块为目标图像块。相比传统特征, 深度神经网络提取的图像卷积特征具有良好的抗干扰能力, 在大规模图像分类比赛中取得巨大的成功。&lt;/p&gt; &lt;p&gt;使用神经网络模拟相关滤波的整个过程.在相关滤波中, 需要保存模板信息并提取搜索区域特征, 因此基于相关滤波思想的网络一般都采用孪生网络 (Siamese Network) 结构, 其中一条支路保存目标模板信息, 另一条支路用于搜索区域特征提取, 最后将两部分特征进行相关操作, 得到响应图像 (Response...</content>
 </entry>

 

</feed>